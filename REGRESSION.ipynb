{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e79dd-f580-4ce3-93d5-92c00d6070fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression Assignment(PW Skill)\n",
    "\n",
    "# 1- What is Simple Linear Regression?\n",
    "\"\"\"\n",
    "Simple Linear Regression is a statistical method used to model the relationship between \n",
    "a dependent variable (Y) and a single independent variable (X). It assumes a linear relationship \n",
    "between the two variables, expressed as Y = mX + c, where m is the slope and c is the intercept.\n",
    "\"\"\"\n",
    "\n",
    "# 2)- What are the key assumptions of Simple Linear Regression?\n",
    "\"\"\"\n",
    "1. Linearity: The relationship between the dependent and independent variable is linear.\n",
    "2. Independence: The residuals (errors) are independent.\n",
    "3. Homoscedasticity: Constant variance of residuals across all levels of the independent variable.\n",
    "4. Normality: Residuals are normally distributed.\n",
    "\"\"\"\n",
    "\n",
    "# 3- What does the coefficient m represent in the equation Y = mX + c?\n",
    "\"\"\"\n",
    "The coefficient m represents the slope of the line. It indicates the change in the dependent variable (Y)\n",
    "for a one-unit increase in the independent variable (X).\n",
    "\"\"\"\n",
    "\n",
    "# 4- What does the intercept c represent in the equation Y = mX + c?\n",
    "\"\"\"\n",
    "The intercept c represents the value of the dependent variable (Y) when the independent variable (X) is zero.\n",
    "It is the point where the regression line crosses the Y-axis.\n",
    "\"\"\"\n",
    "\n",
    "# 5- How do we calculate the slope m in Simple Linear Regression?\n",
    "\"\"\"\n",
    "The slope m is calculated using the formula:\n",
    "    m = Σ((X_i - X_mean) * (Y_i - Y_mean)) / Σ((X_i - X_mean)^2)\n",
    "where X_i and Y_i are individual data points, and X_mean and Y_mean are the means of X and Y, respectively.\n",
    "\"\"\"\n",
    "\n",
    "# 6- What is the purpose of the least squares method in Simple Linear Regression?\n",
    "\"\"\"\n",
    "The least squares method minimizes the sum of the squared differences (residuals) between the observed values \n",
    "and the predicted values from the regression line. This ensures the best fit line for the data.\n",
    "\"\"\"\n",
    "\n",
    "# 7- How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
    "\"\"\"\n",
    "The coefficient of determination (R²) measures the proportion of the variance in the dependent variable \n",
    "that is explained by the independent variable. It ranges from 0 to 1, where a higher value indicates a better fit.\n",
    "\"\"\"\n",
    "\n",
    "# 8- What is Multiple Linear Regression?\n",
    "\"\"\"\n",
    "Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship \n",
    "between a dependent variable (Y) and two or more independent variables (X1, X2, ..., Xn).\n",
    "The equation is: Y = b0 + b1X1 + b2X2 + ... + bnXn.\n",
    "\"\"\"\n",
    "\n",
    "#9- What is the main difference between Simple and Multiple Linear Regression?\n",
    "\"\"\"\n",
    "The main difference is the number of independent variables:\n",
    "- Simple Linear Regression: One independent variable.\n",
    "- Multiple Linear Regression: Two or more independent variables.\n",
    "\"\"\"\n",
    "\n",
    "# 10- What are the key assumptions of Multiple Linear Regression?\n",
    "\"\"\"\n",
    "1. Linearity: The relationship between the dependent variable and each independent variable is linear.\n",
    "2. Independence: Observations are independent.\n",
    "3. Homoscedasticity: Constant variance of residuals.\n",
    "4. Normality: Residuals are normally distributed.\n",
    "5. No Multicollinearity: Independent variables are not highly correlated.\n",
    "\"\"\"\n",
    "\n",
    "# 11- What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
    "\"\"\"\n",
    "Heteroscedasticity occurs when the variance of residuals is not constant across levels of the independent variable(s).\n",
    "It can lead to inefficient estimates and affect the validity of hypothesis tests.\n",
    "\"\"\"\n",
    "\n",
    "# 12)- How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
    "\"\"\"\n",
    "To address multicollinearity:\n",
    "1. Remove or combine highly correlated predictors.\n",
    "2. Use dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
    "3. Apply regularization methods such as Ridge or Lasso regression.\n",
    "\"\"\"\n",
    "\n",
    "# 13- What are some common techniques for transforming categorical variables for use in regression models?\n",
    "\"\"\"\n",
    "1. One-Hot Encoding: Converts categories into binary columns.\n",
    "2. Label Encoding: Assigns numeric labels to categories.\n",
    "3. Dummy Variable Encoding: Similar to One-Hot Encoding but avoids the dummy variable trap.\n",
    "\"\"\"\n",
    "\n",
    "# 14- What is the role of interaction terms in Multiple Linear Regression?\n",
    "\"\"\"\n",
    "Interaction terms capture the combined effect of two or more independent variables on the dependent variable. \n",
    "They are useful when the effect of one variable depends on the level of another variable.\n",
    "\"\"\"\n",
    "\n",
    "# 15- How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
    "\"\"\"\n",
    "- Simple Linear Regression: The intercept is the value of Y when X = 0.\n",
    "- Multiple Linear Regression: The intercept is the value of Y when all independent variables are zero.\n",
    "\"\"\"\n",
    "\n",
    "# 16- What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
    "\"\"\"\n",
    "The slope represents the change in the dependent variable for a one-unit change in the independent variable, \n",
    "holding other variables constant (in multiple regression). It determines the strength and direction of the relationship.\n",
    "\"\"\"\n",
    "\n",
    "# 17- How does the intercept in a regression model provide context for the relationship between variables?\n",
    "\"\"\"\n",
    "The intercept provides the baseline value of the dependent variable when all independent variables are zero.\n",
    "It serves as a reference point for interpreting the model.\n",
    "\"\"\"\n",
    "\n",
    "# 18- What are the limitations of using R² as a sole measure of model performance?\n",
    "\"\"\"\n",
    "1. R² does not indicate whether the model is appropriate.\n",
    "2. It does not account for overfitting in models with many predictors.\n",
    "3. It does not measure the predictive power on unseen data.\n",
    "4. Adjusted R² is a better alternative for multiple regression.\n",
    "\"\"\"\n",
    "\n",
    "# 19- How would you interpret a large standard error for a regression coefficient?\n",
    "\"\"\"\n",
    "A large standard error indicates high variability in the estimate of the coefficient. \n",
    "It suggests that the coefficient may not be reliably estimated.\n",
    "\"\"\"\n",
    "\n",
    "# 20)- How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
    "\"\"\"\n",
    "Heteroscedasticity can be identified by plotting residuals against predicted values. \n",
    "If the spread of residuals increases or decreases systematically, heteroscedasticity is present. \n",
    "Addressing it is important to ensure valid hypothesis tests and efficient estimates.\n",
    "\"\"\"\n",
    "\n",
    "#21 )- What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
    "\"\"\"\n",
    "This indicates that the model may include unnecessary predictors that do not significantly contribute to explaining \n",
    "the variance in the dependent variable. Adjusted R² penalizes for adding irrelevant variables.\n",
    "\"\"\"\n",
    "\n",
    "# 22))- Why is it important to scale variables in Multiple Linear Regression?\n",
    "\"\"\"\n",
    "Scaling variables ensures that all predictors are on a comparable scale, preventing variables with larger magnitudes \n",
    "from disproportionately influencing the model. It is especially important for regularization techniques.\n",
    "\"\"\"\n",
    "\n",
    "# Polynomial Regression and Related Concepts\n",
    "\n",
    "# )- What is polynomial regression?\n",
    "\"\"\"\n",
    "Polynomial regression is a form of regression analysis that models the relationship between the dependent variable \n",
    "and the independent variable as an nth-degree polynomial. The equation is: Y = b0 + b1X + b2X^2 + ... + bnX^n.\n",
    "\"\"\"\n",
    "\n",
    "# )- How does polynomial regression differ from linear regression?\n",
    "\"\"\"\n",
    "Linear regression models a straight-line relationship, while polynomial regression models a curved relationship by \n",
    "involving higher-degree terms of the independent variable.\n",
    "\"\"\"\n",
    "\n",
    "# )- When is polynomial regression used?\n",
    "\"\"\"\n",
    "Polynomial regression is used when the relationship between the dependent and independent variables is non-linear \n",
    "and cannot be captured by a straight line.\n",
    "\"\"\"\n",
    "\n",
    "# )- What is the general equation for polynomial regression?\n",
    "\"\"\"\n",
    "The general equation is:\n",
    "    Y = b0 + b1X + b2X^2 + b3X^3 + ... + bnX^n,\n",
    "where n is the degree of the polynomial.\n",
    "\"\"\"\n",
    "\n",
    "# )#- Can polynomial regression be applied to multiple variables?\n",
    "\"\"\"\n",
    "Yes, polynomial regression can be extended to multiple variables by including interaction and higher-degree terms \n",
    "for each variable.\n",
    "\"\"\"\n",
    "\n",
    "# )- What are the limitations of polynomial regression?\n",
    "\"\"\"\n",
    "1. Overfitting: Higher-degree polynomials can fit noise in the data.\n",
    "2. Complexity: The model becomes difficult to interpret with higher degrees.\n",
    "3. Extrapolation: Predictions outside the data range may be unreliable.\n",
    "\"\"\"\n",
    "\n",
    "# )- What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
    "\"\"\"\n",
    "1. Cross-validation: Evaluate the model's performance on validation data.\n",
    "2. Adjusted R²: Compare models with different degrees.\n",
    "3. Residual plots: Check for patterns in residuals.\n",
    "4. AIC/BIC: Use information criteria to compare models.\n",
    "\"\"\"\n",
    "\n",
    "# - Why is visualization important in polynomial regression?\n",
    "\"\"\"\n",
    "Visualization helps identify non-linear relationships, assess the fit of the model, and detect overfitting or \n",
    "underfitting by examining the alignment of the curve with the data.\n",
    "\"\"\"\n",
    "\n",
    "# - How is polynomial regression implemented in Python?\n",
    "\"\"\"\n",
    "Example implementation in Python:\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create polynomial features\n",
    "degree = 2\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "# Fit the model\n",
    "poly_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = poly_model.predict(X_test)\n",
    "\"\"\"\n",
    ".................................................\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
